{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51aeb084-21ae-4f51-b9ad-05f5ae8b67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf from scratch\n",
    "# also cosine similarity\n",
    "# get these up on lambda func\n",
    "# https://medium.com/bitgrit-data-science-publication/tf-idf-from-scratch-in-python-ea587d003e9e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f3c34-6f2c-4912-9e99-8ddceb790f5f",
   "metadata": {},
   "source": [
    "<h2>Why TF-IDF?</h2>\n",
    "\n",
    "TF-IDF is a way to create vector representatino of a document by averaging all of the document's word weights.\n",
    "\n",
    "TF-IDF tells you how important a single word is in a corpus by assigning it a weight, and down-weighting common words like \"a\", \"and\", and \"the\". (These are stop words anyway, which I'll be removing).\n",
    "\n",
    "<h2>Breaking Down TF-IDF</h2>\n",
    "TF-IDF number (or score) represents the importance of a term within a document relative to a collection of documents (the \"corpus\"). It reflects how significant a word is by considering its frequency in the document and how rare it is across the entire corpus.\n",
    "\n",
    "1. TF (Term Frequency):\n",
    "Measures how often a term appears in a document.\n",
    "\n",
    "* Higher TF = Term appears frequently in the document.\n",
    "\n",
    "\n",
    "2. IDF (Inverse Document Frequency):\n",
    "Measures how rare a term is across all documents in the corpus.\n",
    "\n",
    "* Higher IDF = Term is rare across the corpus.\n",
    "\n",
    "\n",
    "3. TF-IDF Score:\n",
    "The product of TF and IDF.\n",
    "* High TF-IDF score means that the term is frequent in a specific document, but rare in the overall corpus. This makes the term more important.\n",
    "* Low TF-IDF score means that the term is common across many documents or rarely occurs in the given document.\n",
    "\n",
    "<h2>Advantages</h2>\n",
    "\n",
    "* Highlights important terms like \"Python\", \"backend\", etc. instead of common ones like \"the\"\n",
    "* Simple and computationally efficient\n",
    "\n",
    "<h2>Limitations</h2>\n",
    "\n",
    "* TF-IDF doesn't capture context (e.g., \"engineer\" and \"developer\" might be similar in meaning, but treated differently). By the way, do ATS platforms capture context? Or do they simply use something like TFIDF?\n",
    "* It doesn't consider synonyms or semantic relationships\n",
    "* More advanced techniques like word embeddings (Word2Vec, BERT) can capture semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688ec77-96b8-4816-a828-ea67a84224a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5ba5b-eac3-4002-b005-4d837399c029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc7c5b-71e8-4f48-8625-668d8c1f1f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c13d3-cc18-4fe0-a270-2ed117413483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f00e4-58fb-40d6-9591-5387b699c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51bf36-becd-45fe-b458-46376d5713fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4e5d3-089a-493a-8ae1-c8be1644b898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c53c8fe5-a834-4033-82d9-2adc24159c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_doc = \"\"\"\n",
    "Michael Keene\n",
    " (425)-377-3984 # mkeene@alumni.nd.edu ï linkedin.com/michael-james-keene § github.com/mjkeene Technical Skills\n",
    "Programming Languages: Python, Java, Scala, SQL, TypeScript, JavaScript\n",
    "Python Libraries: Pandas, NumPy, scikit-learn, Matplotlib, seaborn\n",
    "AWS Cloud Computing: Lambda, EC2, S3, Redshift, DynamoDB (NoSQL), Glue, IAM, Lake Formation, CDK Other: Git, Bash Scripting, CI/CD, Workflow Orchestration, Jupyter Notebook, PostgreSQL, Automated Testing\n",
    "Experience\n",
    "Personal Sabbatical April 2023 – Present • Utilized this time for traveling, reading, spending time with family, pursuing professional development, and\n",
    "exploring new hobbies and interests.\n",
    "Amazon Web Services February 2021 – March 2023 Software Engineer – AWS Customer Service Seattle, WA Worked on multiple new and existing features within the backend web service that enables the customer service team to access AWS account metadata for troubleshooting. Learned the Agile workflow process, valuable testing techniques, and implementation practices for distributed systems. Utilized continuous integration/continuous deployment (CI/CD) tools.\n",
    "• Architected and implemented fault-tolerant features for a distributed backend API proxy service handling over 2,500 APIs and 1 billion monthly calls, ensuring seamless access to AWS account metadata for customer service.\n",
    "• Engineered a streamlined internal API onboarding process by integrating cross-functional requirements, enhancing the internal web application, and advancing automation initiatives, resulting in a 75% efficiency gain.\n",
    "• Developed a scalable REST API microservice utilizing API Gateway, Lambda functions, and S3 for efficient data storage, significantly reducing deployment times and simplifying the onboarding process for internal APIs.\n",
    "• Utilized Cloud Development Kit (CDK) in Python and TypeScript to orchestrate AWS infrastructure as code, ensuring consistent and scalable AWS resource provisioning.\n",
    "• Successfully completed courses at Amazon’s Machine Learning University, delving into critical areas like A/B testing, data cleaning, ML workflows, and several machine learning algorithms for classification, regression, and natural language processing (NLP). Applied acquired knowledge to enhance decision-making processes.\n",
    "• Actively responded to high-severity events during on-call rotations, using monitoring tools and log analysis to resolve issues quickly. Conducted proactive system maintenance to ensure ongoing stability and reliability.\n",
    "Amazon May 2020 – February 2021 Software Engineering Candidate – Amazon Technical Academy Seattle, WA\n",
    "• Graduated from a company-sponsored, mentored internal training program led by Amazon Software Engineers,\n",
    "designed to teach software engineering skills and computer science fundamentals in a project-based environment.\n",
    "• Mastered core software engineering practices such as automated testing, Git version control, object-oriented\n",
    "programming, software design principles, continuous integration/continuous deployment (CI/CD), data structures\n",
    "and algorithms, and AWS cloud computing.\n",
    "Amazon June 2018 – May 2020 Business Analyst – Amazon Logistics Business Intelligence Seattle, WA\n",
    "• Automated report generation and data visualization for business partners by processing and cleaning large-scale data from Redshift using SQL, Python, Excel, and workflow orchestration tools, ensuring data quality and query efficiency.\n",
    "• Collaborated with the Data Engineering team on data modeling and creating ETL pipelines to ensure data integrity and reliability using Python, internal APIs, workflow orchestration tools, S3, Glue, and Redshift.\n",
    "• Leveraged various data sources, including AWS Athena, S3, Redshift, and internal APIs for data extraction, and used Python scripting for data validation, data quality checks, data cleaning, and merging datasets.\n",
    "• Developed, maintained, and automated daily analytics dashboards and self-service Tableau dashboards.\n",
    "• Analyzed, validated, and aggregated extensive package-level data for the Research Science team, facilitating\n",
    "machine learning model training for truck route optimization with potential savings exceeding $100M.\n",
    "• Designed and launched an intermediate SQL course for our Analytics teams completed by 100+ individuals.\n",
    "Education\n",
    "University of Notre Dame May 2018\n",
    "Bachelor of Arts in Economics, Minor in Sociology (GPA: 3.59 / 4.00) Notre Dame, IN\n",
    "      • Relevant Coursework: Econometrics, Statistics and Probability, Survey Design\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f58f31-8f10-4a65-8a25-420ccac96457",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_doc = \"\"\"\n",
    "Who you are\n",
    "We want to work with teammates based remotely in the USA, who are excited about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration\n",
    "If you’d like to become a backend expert who can unify data, and build systems that scale for both operations and organization, then Fathom is your next big opportunity!\n",
    "2+ years of software engineering experience in a company/production setting\n",
    "Relevant experience developing backend, integrations, data pipelining, infrastructure, etc. projects in a production setting\n",
    "Problem solving skills and first principles thinking\n",
    "Strong computer science principles including: algorithms, databases (SQL and NoSQL), logic, etc\n",
    "Hands-on backend coding and systems design using best practices in a company setting\n",
    "Effective communication and exceptional collaboration skills\n",
    "Desirable\n",
    "\n",
    "Proficiency in coding with python or another modern backend language\n",
    "Expertise with wrangling healthcare data and/or HIPAA\n",
    "Experience with managing large-scale data labelling and acquisition\n",
    "What the job involves\n",
    "We are looking for a Software Engineer (Backend/Data) to work on data products that drive the core of our business\n",
    "Developing data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs\n",
    "Building performant and expressive interfaces to the data\n",
    "Creating infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7373df3c-5875-4889-b999-16edbfa019bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job that does not line up with my resume at all\n",
    "job_doc_2 = \"\"\"\n",
    "Tilray Beverages is comprised of 15+ craft brands across the United States. These beer and beverage brands possess the hallmarks of strong consumer loyalty and further diversify Tilray’s portfolio. The expected sales volume will elevate Tilray Brands to the 5th largest craft beer business position in the U.S., up from the 9th. Tilray Brands also owns Breckenridge Distillery, the award-winning spirits brand and the World’s Best Blended Whisky, and Happy Flower CBD sparkling non-alcoholic cocktails\n",
    "About Tilray Brands Inc.\n",
    "Tilray Brands is a leading global cannabis-lifestyle and consumer packaged goods company with operations in Canada, the United States, Europe, Australia, and Latin America that is changing people's lives for the better – one person at a time – by inspiring and empowering the worldwide community to live their very best life by providing them with products that meet the needs of their mind, body, and soul and invoke a sense of wellbeing. Tilray’s mission is to be the trusted partner for its patients and consumers by providing them with a cultivated experience and health and wellbeing through high-quality, differentiated brands and innovative products. A pioneer in cannabis research, cultivation, and distribution, Tilray’s unprecedented production platform supports over 20 brands in over 20 countries, including comprehensive cannabis offerings, hemp-based foods, and alcoholic beverages.\n",
    "Looking to develop your career at the forefront of a rapidly expanding industry?\n",
    "About the Role:\n",
    "Tilray Beverages is seeking an organized, motivated, creative, and driven lover of the good life to execute on digital marketing initiatives to highlight and add value to our ever-growing portfolio of quality craft beverages. Our ambition is to reinvigorate the craft beer sector, becoming the fastest growing craft beer supplier in the U.S. To achieve this, we need the very best talent on our team. Feel inspired? Then this may be the opportunity for you. If you are a self-starter who knows how to work and play hard, and you have an unwavering passion for grassroots craft beer, grab your kayak and hop in the river with us! Please note this is a:\n",
    "Fixed Term (6-months) contract, with the possibility of an extension\n",
    "Hourly Role - starting at $24.00/hour to 26.50/hour\n",
    "Job Summary:\n",
    "Coordinates and manages all aspects of content development and deployment across existing and emerging digital channels. Will be responsible for content ideation and execution, receiving and executing creative content briefs from other areas of the business, managing external photographers and videographers as needed. This person will have a solid understanding of how each digital and social media channel works and how to optimize content so that it is always engaging.\n",
    "Responsibilities:\n",
    "Day-to-day management of all existing social media channels, including but not limited to:\n",
    "Content ideation\n",
    "Creating various types of compelling content\n",
    "Content calendaring several weeks/months ahead\n",
    "Posting social content\n",
    "Community management in partnership with external agency and brand teams\n",
    "On-going community management and moderation based on strategy provided by social agency, Integrated Customer Experience and brand teams\n",
    "Identifying, on-boarding and retention of a roster of external content creators as needed\n",
    "Staying on the pulse of social/content trends to ensure continued brand relevance and awareness\n",
    "Posting content across social media accounts\n",
    "Leverage insights and analytics provided by external social agency to influence future content ideation\n",
    "Skills/Qualifications:\n",
    "2-3 years previous experience in a content creation; with a beverage brand preferred but not required\n",
    "Previous experience with a social media posting, monitoring and reporting platform (i.e.—Hootsuite, Sprinklr, Sprout, etc.)\n",
    "Proficiency in Photoshop, Illustrator, Canva or similar photo editing software, as well as video editing software such as Lightroom or Premier\n",
    "Ability to manage freelance photographers and videographers as well as be able to be behind the camera to capture product specific, behind the scenes and lifestyle content\n",
    "Ability to straddle both the creative development and engaging a social community\n",
    "Ability to work on-site in order to procure necessary product and facilitate capture of content\n",
    "Passion for craft beer\n",
    "We have a friendly, supportive team with a coaching and mentoring environment. There are real opportunities for future development and progression – this really could be a move towards the exciting career you’ve always wanted!\n",
    "\n",
    "Tilray Brands Inc. is an equal opportunity employer, committed to promoting diversity and inclusion in our workplace. As a Brand Activation Manager at Tilray Beer Division, you will have the opportunity to lead and grow a dynamic team, innovate and experiment with new ingredients and styles, and contribute to the success of one of the top craft breweries in the nation.\n",
    "Accommodations are available for applicants with disabilities throughout the recruitment process. If you require accommodations for interviews or other meetings, please advise when submitting your application.\n",
    "\n",
    "Please note that Tilray does not authorize, engage, or sponsor any consultants, agencies or organizations that seek certain personal or financial information from you (e.g. passwords, login ids, credit card information). High Park does not charge any application, processing or onboarding fee at any stage of the recruitment or hiring process.\n",
    "\n",
    "When replying to emails, please ensure the sender name and email address match exactly. Please also ensure the Reply-To address matches the sending address exactly.\n",
    "If you are concerned about the authenticity of an email, letter, or call purportedly from, for, or on behalf of High Park, please send an email inquiry to infosec@tilray.com\n",
    "#TilrayS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "38659b50-e869-4b04-a8e8-37a56f48cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using builtin library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the TfidfVectorizer with no custom stop words\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on a sample corpus (can't be empty)\n",
    "vectorizer.fit([\"aloha\"])\n",
    "\n",
    "standard_stop_words = vectorizer.get_stop_words()\n",
    "\n",
    "# Custom stop words to add\n",
    "custom_stop_words = [\"looking\", \"seeking\", \"job\", \"position\", \"role\", \"skilled\", \"team\", \"join\", \"apply\", \"experience\"]\n",
    "\n",
    "# Combine standard and custom stop words\n",
    "expanded_stop_words = standard_stop_words.union(custom_stop_words)\n",
    "\n",
    "corpus = [resume_doc, job_doc]\n",
    "\n",
    "titles = [\"resume\", \"job_posting\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=expanded_stop_words, ngram_range=(1, 3))\n",
    "vector = vectorizer.fit_transform(corpus)\n",
    "dict(zip(vectorizer.get_feature_names(), vector.toarray()[0]))\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    vector.toarray(), index=titles, columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "# resume_tfidf = vector[0]\n",
    "# job_posting_tfidf = vector[1]\n",
    "\n",
    "# Might need a different similarity measure, number is always quite low here\n",
    "# cosine_sim = cosine_similarity(resume_tfidf, job_posting_tfidf)\n",
    "# print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf40e7d5-06b4-4ede-8e69-412535a8272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 notre</th>\n",
       "      <th>00 notre dame</th>\n",
       "      <th>100</th>\n",
       "      <th>100 individuals</th>\n",
       "      <th>100 individuals education</th>\n",
       "      <th>100m</th>\n",
       "      <th>100m designed</th>\n",
       "      <th>100m designed launched</th>\n",
       "      <th>2018</th>\n",
       "      <th>...</th>\n",
       "      <th>workflow process valuable</th>\n",
       "      <th>workflows</th>\n",
       "      <th>workflows machine</th>\n",
       "      <th>workflows machine learning</th>\n",
       "      <th>wrangling</th>\n",
       "      <th>wrangling healthcare</th>\n",
       "      <th>wrangling healthcare data</th>\n",
       "      <th>years</th>\n",
       "      <th>years software</th>\n",
       "      <th>years software engineering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resume</th>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.044106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_posting</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.044088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1582 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   00  00 notre  00 notre dame       100  100 individuals  \\\n",
       "resume       0.022053  0.022053       0.022053  0.022053         0.022053   \n",
       "job_posting  0.000000  0.000000       0.000000  0.000000         0.000000   \n",
       "\n",
       "             100 individuals education      100m  100m designed  \\\n",
       "resume                        0.022053  0.022053       0.022053   \n",
       "job_posting                   0.000000  0.000000       0.000000   \n",
       "\n",
       "             100m designed launched      2018  ...  workflow process valuable  \\\n",
       "resume                     0.022053  0.044106  ...                   0.022053   \n",
       "job_posting                0.000000  0.000000  ...                   0.000000   \n",
       "\n",
       "             workflows  workflows machine  workflows machine learning  \\\n",
       "resume        0.022053           0.022053                    0.022053   \n",
       "job_posting   0.000000           0.000000                    0.000000   \n",
       "\n",
       "             wrangling  wrangling healthcare  wrangling healthcare data  \\\n",
       "resume        0.000000              0.000000                   0.000000   \n",
       "job_posting   0.044088              0.044088                   0.044088   \n",
       "\n",
       "                years  years software  years software engineering  \n",
       "resume       0.000000        0.000000                    0.000000  \n",
       "job_posting  0.044088        0.044088                    0.044088  \n",
       "\n",
       "[2 rows x 1582 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb29a68c-660b-4128-8255-06ba540de0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data                           0.313688\n",
       "scale                          0.156844\n",
       "backend                        0.156844\n",
       "setting                        0.132263\n",
       "learning                       0.094107\n",
       "                                 ...   \n",
       "exceeding 100m designed        0.000000\n",
       "exceeding 100m                 0.000000\n",
       "exceeding                      0.000000\n",
       "events rotations using         0.000000\n",
       "javascript python libraries    0.000000\n",
       "Name: job_posting, Length: 1582, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort terms by importance in the job_posting\n",
    "important_job_terms = tfidf_df.loc[\"job_posting\"].sort_values(ascending=False)\n",
    "important_job_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2106034f-eeff-42ba-bfed-4de3b2fb45e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setting                           0.132263\n",
       "ingest                            0.088176\n",
       "developing                        0.088176\n",
       "labelling                         0.088176\n",
       "production                        0.088176\n",
       "production setting                0.088176\n",
       "work                              0.088176\n",
       "build                             0.088176\n",
       "collaboration                     0.088176\n",
       "coding                            0.088176\n",
       "medical                           0.088176\n",
       "databases sql                     0.044088\n",
       "developing data infrastructure    0.044088\n",
       "effective                         0.044088\n",
       "relevant developing               0.044088\n",
       "relevant developing backend       0.044088\n",
       "data work                         0.044088\n",
       "remotely                          0.044088\n",
       "remotely usa                      0.044088\n",
       "remotely usa excited              0.044088\n",
       "Name: job_posting, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify missing terms: Terms that have a non-zero score in 'job_posting' but are 0 in 'resume'\n",
    "missing_terms = important_job_terms[tfidf_df.loc[\"resume\"] == 0]\n",
    "missing_terms.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f13fd-d6aa-4374-9164-d72e99c78830",
   "metadata": {},
   "source": [
    "<h3>Important Updates</h3>\n",
    "Some things that need to be handled to ensure the output is actually useful:\n",
    "\n",
    "1. Remove stopwords, these don't add value (can easily do this with TfidfVectorizer from sklearn). Add custom stop-words to remove common but irrelevant job posting words.\n",
    "2. Word-stemming/Lemmatization? Not going to do this since it will change the inputs, I want exact matching, though ATS systems likely use broad matching in most cases. Maybe a future update.\n",
    "3. <b>Can use n-grams (2 or 3 grams) to capture meaningful sequences of words, rather than just individual words. Do this on top of the 1-gram default. There are so many multi-word, important terms like \"python developer\", \"software engineer\", \"machine learning\", etc. Bigrams and Trigrams will capture more meaningful matches.</b>\n",
    "4. Upgrade to Word2Vec, BERT, or GloVe to capture semantic similarity later on (if that's how ATS systems process resumes; if they use more basic comparisons like tf-idf, then I should stick with this).\n",
    "5. Not really a consideration here, but TF-IDF is skewed by document length. Longer documents may have more common terms, which might result in a lower weight for the truly important keywords. Normalizing tf-idf scores or considering other weighting schemes like BM25 can handle this.\n",
    "7. Update evaluation of matching based on precision/recall -- I want correct matches, rather than catching every single match.\n",
    "8. Can add some kind of intelligent filtering to only show most relevant terms. For now I'll show the top X terms, but it would be better to have a way to dynamically change this (i.e., if only 3 terms missing are relevant, and then the others are low score adjectives/verbs, it's probably best to only show those top 3).\n",
    "9. Add cosine similarity here to get overall similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0b6d8-65f9-4c87-9729-e238494c89c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa7c26-5318-48aa-b0d8-c228a4dab8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b1c2d-e25c-4176-b1e9-50750395c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4af86-2665-4b06-a69e-b53f8b3aaeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cceac21-7579-4d11-997b-88af3b6fea6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b1518-99b2-4e0d-991d-7bccbdb4e01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7485613-0c78-4222-ab7b-00c8ceb2d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
